{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Do not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOA\n",
      "SOS\n",
      "Cardinal\n",
      "Theodore\n",
      "McCarrick\n",
      ":\n",
      "Yet\n",
      "Another\n",
      "Fruit\n",
      "of\n",
      "Vatican\n",
      "II\n",
      "Still\n",
      ",\n",
      "we\n",
      "’re\n",
      "also\n",
      "averse\n",
      "to\n",
      "speaking\n"
     ]
    }
   ],
   "source": [
    "train_words = []\n",
    "with open(\"train_task.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:20]:\n",
    "        print(line.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f844228ddefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatrix_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweights_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwords_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "matrix_len = len(target_vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 50))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(\"/home/raghavan/Downloads/wiki-news-300d-1M-subword.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fb7f3f48b297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-large-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "with open(\"all_task.txt\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    for line in all_lines:\n",
    "        all_words.append(line.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "all_words = list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "not_toknizable = []\n",
    "for word in all_words:\n",
    "    word_tokens = tokenizer.tokenize(word.lower())\n",
    "    tokiziable = True\n",
    "    for w in word_tokens:\n",
    "        if \"##\" in w:\n",
    "            tokiziable = False\n",
    "    if not tokiziable:\n",
    "        not_toknizable.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8943"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_toknizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Soetoro\\n',\n",
       " 'Upend\\n',\n",
       " 'presser',\n",
       " 'dismissive',\n",
       " 'Consortiumnews.com',\n",
       " 'sneaky',\n",
       " 'prosecuting\\n',\n",
       " 'Latina',\n",
       " 'epochal',\n",
       " '701',\n",
       " 'preventative',\n",
       " 'pic.twitter.com/xAVa9TJW70\\n',\n",
       " '2,037',\n",
       " 'multiculturalism',\n",
       " 'expeditiously',\n",
       " 'gratuitous',\n",
       " 'episcopate',\n",
       " 'malfeasance',\n",
       " 'deplorables',\n",
       " 'defaming',\n",
       " 'Arabiya',\n",
       " 'imbecilic\\n',\n",
       " 'alleges',\n",
       " 'analysing',\n",
       " 'Awan',\n",
       " 'nomad\\n',\n",
       " 'Penkoksi',\n",
       " 'virgins',\n",
       " 'appeasing',\n",
       " 'Assata\\n',\n",
       " 'revel',\n",
       " 'cooperating',\n",
       " 'servitors',\n",
       " 'implicate',\n",
       " 'aggresses',\n",
       " 'responsum',\n",
       " 'tactfully\\n',\n",
       " 'Mahony',\n",
       " 'parsec',\n",
       " 'withstood',\n",
       " 'unclued\\n',\n",
       " 'Impersonate\\n',\n",
       " 'quotation',\n",
       " 'pic.twitter.com/z3JRAE80z6',\n",
       " 'graver',\n",
       " 'sideshow',\n",
       " 'Kornmesser',\n",
       " 'https://t.co/MC4Tlvy9gi',\n",
       " 'whistles',\n",
       " 'geniuses',\n",
       " 'Nader',\n",
       " 'incompetents',\n",
       " 'BuzzFeed\\n',\n",
       " 'Bans',\n",
       " 'Baldiserri',\n",
       " 'Ranchers',\n",
       " 'religiously\\n',\n",
       " '@PrisonPlanet',\n",
       " 'wield',\n",
       " '7-JB',\n",
       " 'fecklessness',\n",
       " 'Trumbo',\n",
       " 'Sarkar',\n",
       " 'incarcerate\\n',\n",
       " 'recourses',\n",
       " 'Atta',\n",
       " 'unfold',\n",
       " 'Maurizio',\n",
       " 'Ipsos',\n",
       " 'backslapping',\n",
       " 'Beware',\n",
       " 'harassers',\n",
       " 'SSG',\n",
       " 'antimalarial',\n",
       " 'complying',\n",
       " 'hijabed',\n",
       " '129,656',\n",
       " 'Acta',\n",
       " 'hubris',\n",
       " 'pic.twitter.com/OVGUvozPDZ\\n',\n",
       " 'circumvents\\n',\n",
       " 'outmoded',\n",
       " 'Gugyer\\n',\n",
       " 'embassies',\n",
       " 'Persea',\n",
       " 'Hebda',\n",
       " 'compassionately',\n",
       " 'Bramwell',\n",
       " 'Cortland',\n",
       " 'Leftists',\n",
       " 'escrow',\n",
       " 'perpetrator\\n',\n",
       " 'Carmels',\n",
       " 'Caño',\n",
       " 'ofto',\n",
       " 'bureaucrats',\n",
       " 'Finsbury\\n',\n",
       " 'irrelevancy',\n",
       " 'Eizenkot',\n",
       " 'calledJews']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_toknizable[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "has_new_line = 0\n",
    "for w in not_toknizable:\n",
    "    if \"\\n\" in w:\n",
    "        has_new_line = has_new_line + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1363"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Langenberg\\n'.endswith(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.endswith(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Traning RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "with open(\"all_task.txt\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    for line in all_lines:\n",
    "        all_words.append(line.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476736"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c87e4b3ed5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#             print(vect)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#             print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import bcolz\n",
    "import numpy as np\n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=\"/home/raghavan/Downloads/fasttext\", mode='w')\n",
    "\n",
    "with open(\"/home/raghavan/Downloads/wiki-news-300d-1M-subword.vec\", 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        if line[0] != '999994':\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "#             print(len(vect))\n",
    "#             print(vect)\n",
    "#             print(a)\n",
    "            vectors.append(vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pickle    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((-1, 300)), rootdir=\"/home/raghavan/Downloads/fasttext/6B.50.dat\", mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'/home/raghavan/Downloads/fasttext/6B.50_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'/home/raghavan/Downloads/fasttext/6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "glove_path = \"/home/raghavan/Downloads/fasttext/\"\n",
    "vectors = bcolz.open(f'{glove_path}/6B.50.dat')[:]\n",
    "words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create weight matrix for embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "no_embedding_count = 0\n",
    "target_vocab = list(set(all_words))\n",
    "matrix_len = len(target_vocab)\n",
    "weights_matrix = np.zeros((matrix_len, 300))\n",
    "words_found = 0\n",
    "emb_dim = 300\n",
    "word2idx = {}\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        word_lower = word.lower()\n",
    "        if (word_lower.endswith(\"\\n\")):\n",
    "            word_lower = word_lower.replace(\"\\n\",\"\")\n",
    "        weights_matrix[i] = glove[word_lower]\n",
    "        word2idx[word_lower] = i\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "        no_embedding_count = no_embedding_count + 1\n",
    "        word2idx[word_lower] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "word2idx[\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create pytorch dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,train,dev):\n",
    "        \n",
    "        self.state = \"train\"\n",
    "        \n",
    "        self.train_data =train \n",
    "        self.dev_data = dev\n",
    "        \n",
    "        self.data = self.train_data\n",
    "        \n",
    "    def __getitem__(self,ix):\n",
    "        \n",
    "        return (torch.tensor(self.data[ix][\"x_data\"]).to(device),torch.tensor(self.data[ix][\"y_data\"]).to(device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def set_split(self,state):\n",
    "        self.state = state\n",
    "        if state == \"train\":\n",
    "            self.data = self.train_data\n",
    "        else:\n",
    "            self.data = self.dev_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create Utils functions to create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_data_from_file(file_path):\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"SOA\"):\n",
    "                pass\n",
    "            elif line.startswith(\"SOS\"):\n",
    "                if words:\n",
    "                    examples.append((words,labels))\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                processed_word = splits[0].lower()\n",
    "                if processed_word.endswith(\"\\n\"):\n",
    "                    processed_word = processed_word.replace(\"\\n\",\"\")\n",
    "                words.append(processed_word)\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"O\")\n",
    "        if words:\n",
    "            examples.append((words,labels))\n",
    "    return examples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "labels = {\"I\" : 1 , \"O\":0}\n",
    "max_seq_length =300\n",
    "def words_to_ids(word_label_tup,max_seq_length=300,pad_token=0):\n",
    "    processed_tups = []\n",
    "    for tup in word_label_tup:\n",
    "        ids = []\n",
    "        lbls = []\n",
    "        for word in tup[0]:\n",
    "            ids.append(word2idx[word])\n",
    "        for lbl in tup[1]:\n",
    "            lbls.append(labels[lbl])            \n",
    "        if len(ids) < max_seq_length:\n",
    "            padding_length = max_seq_length - len(ids)\n",
    "            ids += ([pad_token] * padding_length)\n",
    "            lbls += ([-100] * padding_length)\n",
    "        processed_tups.append({\"x_data\":ids,\"y_data\":lbls})\n",
    "    return processed_tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_data = words_to_ids(get_data_from_file(\"train_task.txt\"))\n",
    "dev_data = words_to_ids(get_data_from_file(\"dev_task.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = True\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, weights_matrix, hidden_size, num_layers):\n",
    "#         super(self).__init__()\n",
    "#         self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "#     def forward(self, inp, hidden):\n",
    "#         return self.gru(self.embedding(inp), hidden)\n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, weights_matrix, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(300, hidden_dim, 3, batch_first=True,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim *2, 1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embedding(sentence)\n",
    "#         print(embeds.view(len(sentence), 300, -1).shape)\n",
    "        lstm_out, a = self.lstm(embeds.view(len(sentence), 300, -1))\n",
    "        k = lstm_out.contiguous().view(-1, 600)\n",
    "        tag_space = self.hidden2tag(self.dropout(k))\n",
    "        tag_scores = F.sigmoid(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = Seq2SeqDataset(train_data,None)\n",
    "dataset_dev = Seq2SeqDataset(None,dev_data)\n",
    "dataset_dev.set_split(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dev.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset,batch_size=180,shuffle=False,drop_last=True)\n",
    "dataloader_dev = DataLoader(dataset=dataset_dev,batch_size=180,shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(list(dataloader)[0][1] == -100).nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define few utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report , f1_score, precision_score ,recall_score\n",
    "\n",
    "# def generate_batches(dataset, batch_size, shuffle=True,device=\"0\"):\n",
    "#     dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "#                             shuffle=shuffle)\n",
    "\n",
    "#     for data_d in dataloader:\n",
    "#         out_data_dict = {}\n",
    "#         for name, tensor in data_dict.items():\n",
    "#             out_data_dict[name] = data_dict[name].to(device)\n",
    "#         yield out_data_dict  \n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (y_pred>0.5).cpu().float()#.max(dim=1)[1]\n",
    "    p = precision_score(y_target.cpu().numpy(),y_pred_indices.cpu().detach().numpy())\n",
    "    r = recall_score(y_target.cpu().numpy(),y_pred_indices.cpu().detach().numpy())\n",
    "    f1 = f1_score(y_target.cpu().numpy(),y_pred_indices.cpu().detach().numpy())\n",
    "    return p , r, f1\n",
    "    #n_correct = (y_pred.view(-1)>0.5).sum().cpu().float().item()\n",
    "    #return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Run Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch import optim\n",
    "\n",
    "model = LSTMTagger(weights_matrix,300,300,2).to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "for epoch in range(10):\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "\n",
    "    \n",
    "    running_p = 0\n",
    "    running_r = 0\n",
    "    running_f1 = 0\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    for batch_index,batch in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        y_pred = model(batch[0])\n",
    "        \n",
    "        actual_sentence_mask = (batch[1].view(-1) != -100)\n",
    "        y_pred_mask = y_pred[actual_sentence_mask]\n",
    "        target_mask = batch[1].view(-1)[actual_sentence_mask]\n",
    "\n",
    "        loss = loss_fn(y_pred_mask,target_mask.float())\n",
    "        loss_t = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        p , r , f1 = compute_accuracy(y_pred_mask,target_mask.float())\n",
    "        running_p += (p - running_p) / (batch_index + 1)   \n",
    "        running_r += (r - running_r) / (batch_index + 1) \n",
    "        running_f1 += (f1 - running_f1) / (batch_index + 1) \n",
    "    \n",
    "    running_p_test = 0\n",
    "    running_r_test = 0\n",
    "    running_f1_test = 0\n",
    "    running_loss_test = 0\n",
    "    \n",
    "    print(\"Training :Epoch: %s ,loss: %s ,Precision: %s, Recall %s , F1: %s\" % (epoch,running_loss,running_p,running_r,running_f1))\n",
    "    model.eval()\n",
    "    for batch_index,batch in enumerate(dataloader_dev):\n",
    "        \n",
    "        y_pred = model(batch[0])\n",
    "        \n",
    "        actual_sentence_mask = (batch[1].view(-1) != -100)\n",
    "        y_pred_mask = y_pred[actual_sentence_mask]\n",
    "        target_mask = batch[1].view(-1)[actual_sentence_mask]\n",
    "\n",
    "        loss = loss_fn(y_pred_mask,target_mask.float())\n",
    "        loss_t = loss.item()\n",
    "\n",
    "        running_loss_test += (loss_t - running_loss_test) / (batch_index + 1)\n",
    "\n",
    "        p , r , f1 = compute_accuracy(y_pred_mask,target_mask.float())\n",
    "        running_p_test += (p - running_p_test) / (batch_index + 1)   \n",
    "        running_r_test += (r - running_r_test) / (batch_index + 1) \n",
    "        running_f1_test += (f1 - running_f1_test) / (batch_index + 1) \n",
    "    \n",
    "    \n",
    "    print(\"Dev:  Epoch: %s ,loss: %s ,Precision: %s, Recall %s , F1: %s\" % (epoch,running_loss_test,running_p_test,running_r_test,running_f1_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Prep for test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_data_from_file_for_test(file_path):\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"SOA\"):\n",
    "                pass\n",
    "            elif line.startswith(\"SOS\"):\n",
    "                if words:\n",
    "                    examples.append(words)\n",
    "                    words = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                processed_word = splits[0].lower()\n",
    "                if processed_word.endswith(\"\\n\"):\n",
    "                    processed_word = processed_word.replace(\"\\n\",\"\")\n",
    "                words.append(processed_word)\n",
    "        if words:\n",
    "            examples.append(words)\n",
    "    return examples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "labels = {\"I\" : 1 , \"O\":0}\n",
    "max_seq_length =300\n",
    "def words_to_ids_for_test(word_label_tup,max_seq_length=300,pad_token=0):\n",
    "    processed_tups = []\n",
    "    all_words = []\n",
    "    for ws in word_label_tup:\n",
    "        ids = []\n",
    "        words = []\n",
    "        for word in ws:\n",
    "            ids.append(word2idx[word])    \n",
    "            words.append(word)\n",
    "        if len(ids) < max_seq_length:\n",
    "            padding_length = max_seq_length - len(ids)\n",
    "            ids += ([pad_token] * padding_length)\n",
    "            #words += ([\";lkjh\"] * padding_length)\n",
    "        all_words.append(words)\n",
    "        processed_tups.append({\"x_data\":ids,\"y_data\":[]})\n",
    "    return processed_tups, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_data ,words = words_to_ids_for_test(get_data_from_file_for_test(\"test_task.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_data_from_file_for_test(\"test_task.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_dataset = Seq2SeqDataset(None, test_data)\n",
    "test_dataset.set_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset=test_dataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(list(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sentence_and_prediction = []\n",
    "for batch_index,batch in enumerate(test_dataloader):\n",
    "    \n",
    "    y_pred = model(batch[0])\n",
    "    sentence_and_prediction.append((batch[0].detach().cpu().numpy(),y_pred.detach().cpu().numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(sentence_and_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# inv_weight_matrix = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# sentence_and_prediction_corrent_shape = []\n",
    "# for sentence_prediction in sentence_and_prediction:\n",
    "#     end_index = np.where(sentence_prediction[0][0] == 0)[0][0]\n",
    "#     output_labels = []\n",
    "#     for i in sentence_prediction[1][:end_index].reshape(-1):\n",
    "#         if i > 0.5 :\n",
    "#             label = \"I\"\n",
    "#         else:\n",
    "#             label = \"O\"\n",
    "#         output_labels.append(label)\n",
    "#     input_words = []\n",
    "#     for i in sentence_prediction[0][0][:end_index]:\n",
    "#         input_words.append(inv_weight_matrix[i])\n",
    "        \n",
    "#     sentence_and_prediction_corrent_shape.append((input_words,output_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in sentence_and_prediction:\n",
    "    end_index = np.where(i[0][0] == 0)[0][0]\n",
    "    for i in sentence_prediction[1][:end_index].reshape(-1):\n",
    "        if i > 0.5 :\n",
    "            label = \"I\"\n",
    "        else:\n",
    "            label = \"O\"\n",
    "        predictions.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_strings = []\n",
    "with open(\"test_task.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for ix,line in list(enumerate(lines)):\n",
    "#         print(line)\n",
    "        if (line == '\\n' and (lines[ix-1].startswith(\"EOA\") or lines[ix-1].startswith(\"EOS\")) and ix >1):\n",
    "#         if line.startswith(\"SOA\") or line.startswith(\"SOS\"):\n",
    "            pass\n",
    "        else:\n",
    "            article_strings.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_string_prediction = list(zip(article_strings,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# article_string_prediction = []\n",
    "# for tup in sentence_and_prediction_corrent_shape:\n",
    "#     for i in zip(tup[0],tup[1]):\n",
    "#         article_string_prediction.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_string_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "final_article_string_prediciton = []\n",
    "with open(os.path.join(\"test_task.txt\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    count = 0\n",
    "    for ix, line in enumerate(lines):\n",
    "        if (line == '\\n' and (lines[ix-1].startswith(\"EOA\") or lines[ix-1].startswith(\"EOS\")) and ix >1):\n",
    "            final_article_string_prediciton.append((\"\",\"\"))\n",
    "        else:\n",
    "            final_article_string_prediciton.append((article_string_prediction[count][0].replace(\"\\n\",\"\"),article_string_prediction[count][1].replace(\"\\n\",\"\").replace(\"\\n\",\"\")))\n",
    "            count = count + 1\n",
    "\n",
    "final_string_array = [ i[0] + \" \" + i[1]for i in final_article_string_prediciton]\n",
    "\n",
    "final_string = \"\\n\".join(final_string_array)\n",
    "\n",
    "with open(os.path.join(\"final_prediction.txt\"), \"w\") as f:\n",
    "    f.writelines(final_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# len(%precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_strings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval2020",
   "language": "python",
   "name": "semeval2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
