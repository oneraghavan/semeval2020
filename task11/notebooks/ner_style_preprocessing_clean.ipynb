{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preparing data in the format for ner tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Original input data is in format :\n",
    "* Each input article was given in seperated text file (article id in text file name), for train and dev sets\n",
    "* With the same article id in label file , we get annotation of propaganda spans:\n",
    "    * article_id propaganda start_span end_span\n",
    "\n",
    "    * article id 1111 is present in article_1111.txt and label is present in article_1111_TC.txt \n",
    "         * article_1111.txt : Trump the white president say he likes black people working from him .\n",
    "         * article_1111_TC.txt : \n",
    "             * 1111 <propaganda type> 11 88\n",
    "\n",
    "    \n",
    "From this input we convert we covert the articles to files for each article ids , but in positions of span text , markings of start span and end span.This is done by using the scripts given by the competition organizer. For example : \n",
    "\n",
    " * the output after processing looks like \n",
    "    \n",
    "    article_1111.txt : Trump the <span-7> white president say he likes black people working from him. <7-/span> \n",
    "    \n",
    "    the id 7 comes from type of propaganda. each propaganda has been given a seperated id .\n",
    "    \n",
    "From the above output the following preprocessing turns this article id into seperate sentences each having ner like labels\n",
    "    \n",
    "Example \n",
    "    \n",
    "      file no : article_1111.txt\n",
    "      \n",
    "      Trump O\n",
    "      the O\n",
    "      white I \n",
    "      president I\n",
    "      say I\n",
    "      he I\n",
    "      likes I\n",
    "      black I\n",
    "      people I\n",
    "      working I\n",
    "      from I\n",
    "      him I\n",
    "      . I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Start loading all articles from independent text files into list of tup of (article_id, content)\n",
    "content looks like this \n",
    "\n",
    "\" Trump the white president say he likes black people working from him. <7-/span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import swifter\n",
    "path = r'/data/semeval-2020/task-11/datasets/train-tagged_article/'\n",
    "all_files = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "content_tuple = []\n",
    "\n",
    "for filename in all_files:\n",
    "    file = open(filename)\n",
    "    content = \"\".join(file.readlines()).replace(\"\\n\",\" \")\n",
    "    article_id = filename.split(\"article\")[-1].split(\".txt\")[0]\n",
    "    content_tuple.append((article_id,content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "content_df = pd.DataFrame(content_tuple,columns=[\"article_id\",\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>762956953</td>\n",
       "      <td>Iran Admits To Aiding Al-Qaeda and Facilitatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>787529309</td>\n",
       "      <td>The Last-Minute &lt;span-11 Character Assassinati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>999001296</td>\n",
       "      <td>Altered Election Documents Tied To Florida Dem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                            content\n",
       "0  762956953  Iran Admits To Aiding Al-Qaeda and Facilitatin...\n",
       "1  787529309  The Last-Minute <span-11 Character Assassinati...\n",
       "2  999001296  Altered Election Documents Tied To Florida Dem..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now we write methods to help us convert the above text into ner style tagging .\n",
    "\n",
    "get_propaganda_sequence :\n",
    "    Takes in a each row of content_df and outputs for each words in the sentence into :\n",
    "    \n",
    "        * (word1,propaganda_id) if the word is in a span of propaganda \n",
    "        or \n",
    "        * (word1,0) if the word is not in a span of propaganda \n",
    "        \n",
    "    There is no preprocessing done on each word / text , only the span marking such as <span-pid and pid-/span> are removed\n",
    "    Only extra thing we do is we mark end of every sentence by appending the last word of the sentence with $$$$$ , while writing out as a file ,       we will remove these marking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English   # updated , \n",
    "from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def get_technique_id(token):\n",
    "    results = {\"start_token\":[],\"end_token\":[]}\n",
    "\n",
    "    if \"<span\" in token:\n",
    "        results[\"start_token\"] = [i.replace(\"<span-\",\"\") for i in re.findall(\"<span-\\d+\",token)]\n",
    "\n",
    "    if \"-/span>\" in token:\n",
    "        results[\"end_token\"] = [i.replace(\"-/span>\",\"\") for i in re.findall(\"\\d+-/span>\",token)]\n",
    "\n",
    "    # return [res for res_array in results_start_span for res in res_array]\n",
    "    return results\n",
    "\n",
    "def get_propaganda_sequence(row):\n",
    "#     print(content)\n",
    "    content_lable_tup = []\n",
    "    content = row.content.replace(\"><\", \"> <\").replace(\">“<\", \"> <\") \\\n",
    "        .replace(\">(<\", \"> <\").replace(\">.<\", \"> <\") \\\n",
    "        .replace(\">\\'<\", \"> <\")\n",
    "    label_seq = []\n",
    "    label_propaganda_seq = []\n",
    "    running_token = []\n",
    "    for sent in nlp(content).sents:\n",
    "        len_of_sentences = len(str(sent).split(\" \"))\n",
    "        tokens = sent.text.split(\" \")\n",
    "        for ix,token in enumerate(tokens):\n",
    "            token = token.strip()\n",
    "            modified_token = token\n",
    "            if \"span\" in token:\n",
    "                if \"-/span>\" in token:\n",
    "                    for i in get_technique_id(token)[\"end_token\"]:\n",
    "                        running_token.remove(i)\n",
    "                    running_token = sorted(running_token)\n",
    "\n",
    "                    rest_of_token = re.sub(\"\\d+-/span>\", \"\", token.strip())\n",
    "                    if len(rest_of_token) > 1:\n",
    "                        split_toks = [i.text for i in nlp(rest_of_token)]\n",
    "                        for tok in split_toks:\n",
    "                            if len(running_token) == 0:\n",
    "                                label_propaganda_seq.append(\"O\")\n",
    "                                content_lable_tup.append((tok,\"O\"))\n",
    "                            else:\n",
    "                                label_propaganda_seq.append(\",\".join(running_token))\n",
    "                                content_lable_tup.append((tok, \",\".join(running_token)))\n",
    "\n",
    "                if \"<span\" in token:\n",
    "                    running_token = running_token + get_technique_id(token)[\"start_token\"]\n",
    "                    running_token = sorted(running_token)\n",
    "\n",
    "                    rest_of_token = re.sub(\"<span-\\d+\", \"\", token.strip())\n",
    "                    if len(rest_of_token) > 1:\n",
    "                        split_toks = [i.text for i in nlp(rest_of_token)]\n",
    "                        for tok in split_toks:\n",
    "                            if len(running_token) == 0:\n",
    "                                label_propaganda_seq.append(\"O\")\n",
    "                                content_lable_tup.append((tok, \"O\"))\n",
    "                            else:\n",
    "                                label_propaganda_seq.append(\",\".join(running_token))\n",
    "                                content_lable_tup.append((tok, \",\".join(running_token)))\n",
    "\n",
    "            else:\n",
    "                if len(token) > 0:\n",
    "                    split_toks = [i.text for i in nlp(token)]\n",
    "                    for tok in split_toks:\n",
    "                        if len(running_token) == 0:\n",
    "                            label_propaganda_seq.append(\"O\")\n",
    "                            content_lable_tup.append((tok, \"O\"))\n",
    "                        else:\n",
    "                            label_propaganda_seq.append(\",\".join(running_token))\n",
    "                            content_lable_tup.append((tok, \",\".join(running_token)))\n",
    "            if ix == len_of_sentences -1:\n",
    "                content_lable_tup[-1] = (content_lable_tup[-1][0] + \"$$$$$$\",content_lable_tup[-1][1])\n",
    "                    \n",
    "    new_content = [(re.sub(\"<span-\\d+\", \"\", re.sub(\"\\d+-/span>\", \"\", tup[0])) , tup[1]) for tup in content_lable_tup]\n",
    "\n",
    "    seq_to_return = [i for i in new_content if i[0] != \"\"]\n",
    "\n",
    "    return seq_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sample example of what this function does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Farrakhan Speech:<span-11  \\'Jews Are My Enemy,\\'  11-/span>\\'<span-9 White Folks 9-/span> Are Going Down\\'  With the <span-9 leftist media 9-/span> entirely focused on the push to ban AR-15s and repeal the Second Amendment, practically no one noticed Louis Farrakhan\\'s Saviours’ Day 2018 Address in which he told an approving audience that \"<span-11 powerful Jews are my enemy,\" 11-/span> and \"<span-9 white folks 9-/span> are going down,\" according to The Washington Examiner. Farrakhan, of course, is the <span-9 raging anti-Semite and race-monger 9-/span> who leads the Nation of Islam, the <span-9 loony, militant, black nationalist organization 9-/span> whose mission is to throw off the yoke of <span-9 the inferior white devil. 9-/span> This is the same Farrakhan with whom then-Sen. Barack Obama took a photo at a 2005 Congressional Black Caucus meeting, <span-13 a photo that was subsequently suppressed in order to protect Obama\\'s political future.  13-/span>“Jews were responsible for all of this <span-8 filth and degenerate behavior 8-/span> that Hollywood is putting out, turning men into women and women into men,” Farrakhan said in his keynote speech. “<span-8 White folks are going down. And Satan is going down. 8-/span> And Farrakhan, by God’s grace, has pulled a cover off of that <span-9 Satanic Jew, 9-/span> and I’m here to say <span-6 <span-8 your time is up, your world is through.\" 6-/span> 8-/span>After Farrakhan\\'s speech Sunday, CNN anchor Jake Tapper, among a few others, began tweeting out a few quotes from it, and declared that Farrakhan was more dangerous than other \"<span-9 alt-reich\" leaders 9-/span> because <span-2 he “has a much larger following and elected officials meet with him openly.” 2-/span> His Twitter thread begins here: Somehow the <span-9 openly racist and anti-Semitic Farrakhan 9-/span> and his <span-9 hateful organization 9-/span> have managed for decades to avoid being <span-8 harshly denounced 8-/span> as such by the news media, <span-13 which instead has spent the last two years attempting <span-2 to smear Donald Trump as the new Hitler. 13-/span> 2-/span>For more on Farrakhan, check out his profile here at Discover the Networks, the Horowitz Freedom Center\\'s resource site of the left. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df[content_df[\"article_id\"] == \"736231219\"].iloc[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Farrakhan', 'O'),\n",
       " ('Speech', '11'),\n",
       " (':', '11'),\n",
       " (\"'\", '11'),\n",
       " ('Jews', '11'),\n",
       " ('Are', '11'),\n",
       " ('My', '11'),\n",
       " ('Enemy', '11'),\n",
       " (',', '11'),\n",
       " (\"'\", '11'),\n",
       " ('White', '9'),\n",
       " ('Folks', '9'),\n",
       " ('Are', 'O'),\n",
       " ('Going', 'O'),\n",
       " ('Down', 'O'),\n",
       " (\"'\", 'O'),\n",
       " ('With', 'O'),\n",
       " ('the', 'O'),\n",
       " ('leftist', '9'),\n",
       " ('media', '9'),\n",
       " ('entirely', 'O'),\n",
       " ('focused', 'O'),\n",
       " ('on', 'O'),\n",
       " ('the', 'O'),\n",
       " ('push', 'O'),\n",
       " ('to', 'O'),\n",
       " ('ban', 'O'),\n",
       " ('AR-15s', 'O'),\n",
       " ('and', 'O'),\n",
       " ('repeal', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Second', 'O'),\n",
       " ('Amendment', 'O'),\n",
       " (',', 'O'),\n",
       " ('practically', 'O'),\n",
       " ('no', 'O'),\n",
       " ('one', 'O'),\n",
       " ('noticed', 'O'),\n",
       " ('Louis', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('Saviours', 'O'),\n",
       " ('’', 'O'),\n",
       " ('Day', 'O'),\n",
       " ('2018', 'O'),\n",
       " ('Address', 'O'),\n",
       " ('in', 'O'),\n",
       " ('which', 'O'),\n",
       " ('he', 'O'),\n",
       " ('told', 'O'),\n",
       " ('an', 'O'),\n",
       " ('approving', 'O'),\n",
       " ('audience', 'O'),\n",
       " ('that', 'O'),\n",
       " ('powerful', '11'),\n",
       " ('Jews', '11'),\n",
       " ('are', '11'),\n",
       " ('my', '11'),\n",
       " ('enemy', '11'),\n",
       " (',', '11'),\n",
       " ('\"', '11'),\n",
       " ('and', 'O'),\n",
       " ('white', '9'),\n",
       " ('folks', '9'),\n",
       " ('are', 'O'),\n",
       " ('going', 'O'),\n",
       " ('down', 'O'),\n",
       " (',', 'O'),\n",
       " ('\"', 'O'),\n",
       " ('according', 'O'),\n",
       " ('to', 'O'),\n",
       " ('The', 'O'),\n",
       " ('Washington', 'O'),\n",
       " ('Examiner', 'O'),\n",
       " ('.$$$$$$', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " (',', 'O'),\n",
       " ('of', 'O'),\n",
       " ('course', 'O'),\n",
       " (',', 'O'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('raging', '9'),\n",
       " ('anti', '9'),\n",
       " ('-', '9'),\n",
       " ('Semite', '9'),\n",
       " ('and', '9'),\n",
       " ('race', '9'),\n",
       " ('-', '9'),\n",
       " ('monger', '9'),\n",
       " ('who', 'O'),\n",
       " ('leads', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Nation', 'O'),\n",
       " ('of', 'O'),\n",
       " ('Islam', 'O'),\n",
       " (',', 'O'),\n",
       " ('the', 'O'),\n",
       " ('loony', '9'),\n",
       " (',', '9'),\n",
       " ('militant', '9'),\n",
       " (',', '9'),\n",
       " ('black', '9'),\n",
       " ('nationalist', '9'),\n",
       " ('organization', '9'),\n",
       " ('whose', 'O'),\n",
       " ('mission', 'O'),\n",
       " ('is', 'O'),\n",
       " ('to', 'O'),\n",
       " ('throw', 'O'),\n",
       " ('off', 'O'),\n",
       " ('the', 'O'),\n",
       " ('yoke', 'O'),\n",
       " ('of', 'O'),\n",
       " ('the', '9'),\n",
       " ('inferior', '9'),\n",
       " ('white', '9'),\n",
       " ('devil', '9'),\n",
       " ('.$$$$$$', '9'),\n",
       " ('This', 'O'),\n",
       " ('is', 'O'),\n",
       " ('the', 'O'),\n",
       " ('same', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " ('with', 'O'),\n",
       " ('whom', 'O'),\n",
       " ('then', 'O'),\n",
       " ('-', 'O'),\n",
       " ('Sen', 'O'),\n",
       " ('.$$$$$$', 'O'),\n",
       " ('Barack', 'O'),\n",
       " ('Obama', 'O'),\n",
       " ('took', 'O'),\n",
       " ('a', 'O'),\n",
       " ('photo', 'O'),\n",
       " ('at', 'O'),\n",
       " ('a', 'O'),\n",
       " ('2005', 'O'),\n",
       " ('Congressional', 'O'),\n",
       " ('Black', 'O'),\n",
       " ('Caucus', 'O'),\n",
       " ('meeting', 'O'),\n",
       " (',', 'O'),\n",
       " ('a', '13'),\n",
       " ('photo', '13'),\n",
       " ('that', '13'),\n",
       " ('was', '13'),\n",
       " ('subsequently', '13'),\n",
       " ('suppressed', '13'),\n",
       " ('in', '13'),\n",
       " ('order', '13'),\n",
       " ('to', '13'),\n",
       " ('protect', '13'),\n",
       " ('Obama', '13'),\n",
       " (\"'s\", '13'),\n",
       " ('political', '13'),\n",
       " ('future', '13'),\n",
       " ('.$$$$$$', '13'),\n",
       " ('“', 'O'),\n",
       " ('Jews', 'O'),\n",
       " ('were', 'O'),\n",
       " ('responsible', 'O'),\n",
       " ('for', 'O'),\n",
       " ('all', 'O'),\n",
       " ('of', 'O'),\n",
       " ('this', 'O'),\n",
       " ('filth', '8'),\n",
       " ('and', '8'),\n",
       " ('degenerate', '8'),\n",
       " ('behavior', '8'),\n",
       " ('that', 'O'),\n",
       " ('Hollywood', 'O'),\n",
       " ('is', 'O'),\n",
       " ('putting', 'O'),\n",
       " ('out', 'O'),\n",
       " (',', 'O'),\n",
       " ('turning', 'O'),\n",
       " ('men', 'O'),\n",
       " ('into', 'O'),\n",
       " ('women', 'O'),\n",
       " ('and', 'O'),\n",
       " ('women', 'O'),\n",
       " ('into', 'O'),\n",
       " ('men', 'O'),\n",
       " (',', 'O'),\n",
       " ('”', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " ('said', 'O'),\n",
       " ('in', 'O'),\n",
       " ('his', 'O'),\n",
       " ('keynote', 'O'),\n",
       " ('speech', 'O'),\n",
       " ('.', 'O'),\n",
       " ('“$$$$$$', 'O'),\n",
       " ('White', '8'),\n",
       " ('folks', '8'),\n",
       " ('are', '8'),\n",
       " ('going', '8'),\n",
       " ('down', '8'),\n",
       " ('.$$$$$$', '8'),\n",
       " ('And', '8'),\n",
       " ('Satan', '8'),\n",
       " ('is', '8'),\n",
       " ('going', '8'),\n",
       " ('down', '8'),\n",
       " ('.$$$$$$', '8'),\n",
       " ('And', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " (',', 'O'),\n",
       " ('by', 'O'),\n",
       " ('God', 'O'),\n",
       " ('’s', 'O'),\n",
       " ('grace', 'O'),\n",
       " (',', 'O'),\n",
       " ('has', 'O'),\n",
       " ('pulled', 'O'),\n",
       " ('a', 'O'),\n",
       " ('cover', 'O'),\n",
       " ('off', 'O'),\n",
       " ('of', 'O'),\n",
       " ('that', 'O'),\n",
       " ('Satanic', '9'),\n",
       " ('Jew', '9'),\n",
       " (',', '9'),\n",
       " ('and', 'O'),\n",
       " ('I', 'O'),\n",
       " ('’m', 'O'),\n",
       " ('here', 'O'),\n",
       " ('to', 'O'),\n",
       " ('say', 'O'),\n",
       " ('your', '6,8'),\n",
       " ('time', '6,8'),\n",
       " ('is', '6,8'),\n",
       " ('up', '6,8'),\n",
       " (',', '6,8'),\n",
       " ('your', '6,8'),\n",
       " ('world', '6,8'),\n",
       " ('is', '6,8'),\n",
       " ('through', '6,8'),\n",
       " ('.', '6,8'),\n",
       " ('\"$$$$$$', '6,8'),\n",
       " ('After', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('speech', 'O'),\n",
       " ('Sunday', 'O'),\n",
       " (',', 'O'),\n",
       " ('CNN', 'O'),\n",
       " ('anchor', 'O'),\n",
       " ('Jake', 'O'),\n",
       " ('Tapper', 'O'),\n",
       " (',', 'O'),\n",
       " ('among', 'O'),\n",
       " ('a', 'O'),\n",
       " ('few', 'O'),\n",
       " ('others', 'O'),\n",
       " (',', 'O'),\n",
       " ('began', 'O'),\n",
       " ('tweeting', 'O'),\n",
       " ('out', 'O'),\n",
       " ('a', 'O'),\n",
       " ('few', 'O'),\n",
       " ('quotes', 'O'),\n",
       " ('from', 'O'),\n",
       " ('it', 'O'),\n",
       " (',', 'O'),\n",
       " ('and', 'O'),\n",
       " ('declared', 'O'),\n",
       " ('that', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " ('was', 'O'),\n",
       " ('more', 'O'),\n",
       " ('dangerous', 'O'),\n",
       " ('than', 'O'),\n",
       " ('other', 'O'),\n",
       " ('alt', '9'),\n",
       " ('-', '9'),\n",
       " ('reich', '9'),\n",
       " ('\"', '9'),\n",
       " ('leaders', '9'),\n",
       " ('because', 'O'),\n",
       " ('he', '2'),\n",
       " ('“', '2'),\n",
       " ('has', '2'),\n",
       " ('a', '2'),\n",
       " ('much', '2'),\n",
       " ('larger', '2'),\n",
       " ('following', '2'),\n",
       " ('and', '2'),\n",
       " ('elected', '2'),\n",
       " ('officials', '2'),\n",
       " ('meet', '2'),\n",
       " ('with', '2'),\n",
       " ('him', '2'),\n",
       " ('openly', '2'),\n",
       " ('.', '2'),\n",
       " ('”$$$$$$', '2'),\n",
       " ('His', 'O'),\n",
       " ('Twitter', 'O'),\n",
       " ('thread', 'O'),\n",
       " ('begins', 'O'),\n",
       " ('here', 'O'),\n",
       " (':', 'O'),\n",
       " ('Somehow', 'O'),\n",
       " ('the', 'O'),\n",
       " ('openly', '9'),\n",
       " ('racist', '9'),\n",
       " ('and', '9'),\n",
       " ('anti', '9'),\n",
       " ('-', '9'),\n",
       " ('Semitic', '9'),\n",
       " ('Farrakhan', '9'),\n",
       " ('and', 'O'),\n",
       " ('his', 'O'),\n",
       " ('hateful', '9'),\n",
       " ('organization', '9'),\n",
       " ('have', 'O'),\n",
       " ('managed', 'O'),\n",
       " ('for', 'O'),\n",
       " ('decades', 'O'),\n",
       " ('to', 'O'),\n",
       " ('avoid', 'O'),\n",
       " ('being', 'O'),\n",
       " ('harshly', '8'),\n",
       " ('denounced', '8'),\n",
       " ('as', 'O'),\n",
       " ('such', 'O'),\n",
       " ('by', 'O'),\n",
       " ('the', 'O'),\n",
       " ('news', 'O'),\n",
       " ('media', 'O'),\n",
       " (',', 'O'),\n",
       " ('which', '13'),\n",
       " ('instead', '13'),\n",
       " ('has', '13'),\n",
       " ('spent', '13'),\n",
       " ('the', '13'),\n",
       " ('last', '13'),\n",
       " ('two', '13'),\n",
       " ('years', '13'),\n",
       " ('attempting', '13'),\n",
       " ('to', '13,2'),\n",
       " ('smear', '13,2'),\n",
       " ('Donald', '13,2'),\n",
       " ('Trump', '13,2'),\n",
       " ('as', '13,2'),\n",
       " ('the', '13,2'),\n",
       " ('new', '13,2'),\n",
       " ('Hitler', '13,2'),\n",
       " ('.$$$$$$', '13,2'),\n",
       " ('For', 'O'),\n",
       " ('more', 'O'),\n",
       " ('on', 'O'),\n",
       " ('Farrakhan', 'O'),\n",
       " (',', 'O'),\n",
       " ('check', 'O'),\n",
       " ('out', 'O'),\n",
       " ('his', 'O'),\n",
       " ('profile', 'O'),\n",
       " ('here', 'O'),\n",
       " ('at', 'O'),\n",
       " ('Discover', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Networks', 'O'),\n",
       " (',', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Horowitz', 'O'),\n",
       " ('Freedom', 'O'),\n",
       " ('Center', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('resource', 'O'),\n",
       " ('site', 'O'),\n",
       " ('of', 'O'),\n",
       " ('the', 'O'),\n",
       " ('left', 'O'),\n",
       " ('.$$$$$$', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self,content):\n",
    "        self.content = content\n",
    "\n",
    "sample_row = A(content_df[content_df[\"article_id\"] == \"736231219\"].iloc[0].content)\n",
    "        \n",
    "get_propaganda_sequence(sample_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The 2nd helper function get_technique_id get all start token and end token from a given span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_token': ['11'], 'end_token': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_technique_id(\"<span-11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next we use the helper function to do it for all articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghavan/anaconda3/envs/semeval_2020/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cea8fdd3dc4fc2ab7567a70c331792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=371, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "content_df[\"tagged_sequence_with_propaganda_types\"] = content_df.swifter.apply(get_propaganda_sequence,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'O'),\n",
       " ('Last', 'O'),\n",
       " ('-', 'O'),\n",
       " ('Minute', 'O'),\n",
       " ('Character', '11'),\n",
       " ('Assassination', '11'),\n",
       " ('of', 'O'),\n",
       " ('Judge', 'O'),\n",
       " ('Kavanaugh', 'O'),\n",
       " ('Using', 'O')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df[\"tagged_sequence_with_propaganda_types\"][1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As the first task in the competitoin is just to identify the start and end of span , we write a function to convert the about output into just propaganda or not \n",
    "\n",
    "Example a entry :\n",
    "\n",
    " (\"Trump\",11) turns into (\"Trump\",I)\n",
    " \n",
    " where 11 is the propaganda id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def propaganda_type_seq_to_simple_yes_or_no_seq(taggings):\n",
    "    simple_taggings = []\n",
    "    for i in taggings:\n",
    "        if i[1] != \"O\":\n",
    "            simple_taggings.append((i[0],\"I\"))\n",
    "        else:\n",
    "            simple_taggings.append(i)\n",
    "    return simple_taggings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def propaganda_type_seq_to_simple_yes_or_no_seq_bio(taggings):\n",
    "    simple_taggings = []\n",
    "    for ix,i in enumerate(taggings):\n",
    "        if i[1] != \"O\" and taggings[ix-1][1] == \"O\" and ix > 1:\n",
    "            simple_taggings.append((i[0],\"B-I\"))\n",
    "        elif i[1] != \"O\":    \n",
    "            simple_taggings.append((i[0],\"I\"))\n",
    "        else:\n",
    "            simple_taggings.append(i)\n",
    "    return simple_taggings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c76817691e4ce8b8da367f867b645a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=371, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "content_df[\"tagged_sequence\"] = content_df.tagged_sequence_with_propaganda_types.swifter.apply(propaganda_type_seq_to_simple_yes_or_no_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Then we split the the above rows of articles into train and test to be used in our training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df , test_df =  train_test_split(content_df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_tag_array = train_df.tagged_sequence.values.tolist()\n",
    "test_tag_array = test_df.tagged_sequence.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We now create file for each of test and train , Only thing we additionaly do is remove $$$$$ we have added earlier for sentence end markings . Plus in the text file we add SOA when a new article starts and EOA when an article ends , we also add SOS at start of sentence and EOS at end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_string = \"\"\n",
    "count = len(train_tag_array)\n",
    "for i in train_tag_array:\n",
    "    final_string += \"SOA O\\nSOS O\\n\"\n",
    "    for k in i:\n",
    "        final_string = final_string + k[0].replace(\"$$$$$$\",\"\") + \" \" + k[1] + \"\\n\"\n",
    "        if \"$$$$$$\" in k[0]:\n",
    "            final_string = final_string + \"EOS O\\n\\nSOS O\\n\"\n",
    "    final_string += \"EOA O\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('train_task.txt','w') as f:\n",
    "    f.write(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_final_string = \"\"\n",
    "for i in test_tag_array:\n",
    "    test_final_string += \"SOA O\\nSOS O\\n\"\n",
    "    for k in i:\n",
    "        test_final_string = test_final_string + k[0].replace(\"$$$$$$\",\"\") + \" \" + k[1] + \"\\n\"\n",
    "        if \"$$$$$$\" in k[0]:\n",
    "            test_final_string = test_final_string + \"EOS O\\n\\nSOS O\\n\"\n",
    "    test_final_string += \"EOA O\\n\\n\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('dev_task.txt','w') as f:\n",
    "    f.write(test_final_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Prepare test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now for preparing the test set , we just split article into words and add SOA , EOA , SOS , EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dev_articles_path = \"/data/semeval-2020/task-11/datasets/dev-articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import swifter\n",
    "all_files = glob.glob(dev_articles_path + \"/*.txt\")\n",
    "\n",
    "dev_content_tuple = []\n",
    "\n",
    "for filename in all_files:\n",
    "    file = open(filename)\n",
    "    content = \"\".join(file.readlines()).replace(\"\\n\",\" \")\n",
    "    article_id = filename.split(\"article\")[-1].split(\".txt\")[0]\n",
    "    dev_content_tuple.append((article_id,content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('779309765',\n",
       " \"Unbelievable! Sharia New Mexico: Islamic compound jihadis RELEASED on bond after charges of “Islamophobia” and “racism” (Islam is not a race)  Editor's Note: Talk about injustice! There were remains of a 4-year-old boy found there, who allegedly died while they were performing some sort of Islamic ritual over him. The Bundys and their supporters got nearly two years in jail and no one hurt a single person! Shame on those who are supposed to uphold law and justice! Shame! Taos County Sheriff Jerry Hogrefe testified that they found children holding boxes of ammunition, and that one child was found with a gun…. Because sharia trumps dead children, school shooting training, kidnapping and jihad training. The children discovered at an “extremist Muslim” compound in New Mexico earlier this month were both trained to use firearms and taught multiple tactical techniques in order to kill teachers, law enforcement and other institution…. state prosecutors said on Monday. take our poll - story continues below Will Brett Kavanaugh be confirmed to the Supreme Court? Will Brett Kavanaugh be confirmed to the Supreme Court? Will Brett Kavanaugh be confirmed to the Supreme Court? * Yes, he will be confirmed. No, he will not be confirmed. Email * Comments This field is for validation purposes and should be left unchanged. Completing this poll grants you access to Freedom Outpost updates free of charge. You may opt out at anytime. You also agree to this site's Privacy Policy and Terms of Use. “Islamophobia” is a thought crushing device designed to silence any and critics and criticism of Islam. It is the tool in which the ummah enforces sharia blasphemy laws in the West. Waco – the Feds killed ’em all. Shot it up and burned it down. Fast forward to Islam in America — New Mexico compound jihadis released on bond after charges of racism and “Islamophobia” “THEIR LAWYER ARGUED THAT THERE WAS A DOUBLE STANDARD IN THE CASE BECAUSE HIS CLIENTS WERE MUSLIMS. HE ARGUED THAT IF THEY HAD BEEN CHRISTIAN AND WHITE, ‘WE MIGHT NOT BE HERE TODAY.’” Bu Jihad Watch, August 13, 2018: Seriously? If they had been Christian and white, stockpiling weapons and plotting school shootings, with the ringleader being the son of one of the most respected Christian preachers in the country, there wouldn’t have been news coverage of anything else for weeks. There would have been feature stories in the New York Times, the Washington Post, and the Wall Street Journal. CNN and MSNBC would be running special reports on toxic Christianity and the crisis in the churches. This incident, on the other hand, received very little coverage, and hardly any that touched on the Islamic aspects of the story. But Muslim claims of victimhood are so polished and reflexive by now, and the Leftist acceptance of them so matter-of-fact and instinctive, that Judge Sarah Backus fell right into line. We can only hope that no one dies because of her folly. “Breaking: Judge makes stunning decision in Muslim compound case after charges of racism,” by Carlos Garcia, The Blaze, August 13, 2018 (thanks to Robert): In a hearing Monday, a New Mexico judge found that suspects in a bizarre child abuse case were not a danger to the public, and released them on bond. Defense attorney blames Islamophobia Two Muslim men and three women are charged with 11 counts of child abuse each after police raided their compound in New Mexico and discovered 11 malnourished children living in squalor. Their lawyer argued that there was a double standard in the case because his clients were Muslims. He argued that if they had been Christian and white, “we might not be here today.” Despite law enforcement authorities saying that they believe that the children were being trained to attack innocents, and that the adults were radicalized, Judge Sarah Backus released the suspects. Prosecutors also said they obtained a letter from one of the suspects telling his brother to come to the compound and die like a martyr. The lawyer for the suspects said that there was no evidence that they were planning any kind of attacks. Taos County Sheriff Jerry Hogrefe testified that they found children holding boxes of ammunition, and that one child was found with a gun… Article posted with permission from Pamela Geller \")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_content_tuple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_content_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English # updated\n",
    "from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "\n",
    "article_id_sentence = [(i[0],str(k)) for i in dev_content_tuple for k in nlp(i[1]).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dev_article_text = \"\"\n",
    "for i in dev_content_tuple:\n",
    "    dev_article_text += \"SOA\\n\"\n",
    "    for sent in nlp(i[1]).sents:\n",
    "        dev_article_text += \"SOS\\n\"\n",
    "        tokens = str(sent).split(\" \")\n",
    "#         tokens = tokenizer.tokenize(str(sent))\n",
    "        for token in tokens:\n",
    "            for tok in nlp(token):\n",
    "                dev_article_text = dev_article_text + tok.text + \"\\n\"\n",
    "        dev_article_text = dev_article_text + \"EOS\\n\\n\"\n",
    "    dev_article_text = dev_article_text + \"EOA\\n\\n\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# dev_article_text = \"\"\n",
    "# for combo in article_id_sentence:\n",
    "#     dev_article_text += \"SOA O\\n\"\n",
    "#     for token in combo[1].split(\" \"):\n",
    "#         dev_article_text = dev_article_text + token + \"\\n\"\n",
    "#     dev_article_text = dev_article_text + \"\\n\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open('test_task.txt',\"w\") as f:\n",
    "    f.write(dev_article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOA\\nSOS\\nUnbelievable\\n!\\nEOS'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_article_text.split(\"\\n\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Post processing for bert transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Out of any model will be similar where each line will contain word and label (I or O) seperated by a tab. We have to covert it into a article again using SOA , SOS , EOS and EOA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_prediction = open(\"final_prediction.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_ids = [tup[0] for tup in dev_content_tuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_prediction = []\n",
    "running_article = []\n",
    "for line in final_prediction:\n",
    "    if line.startswith(\"SOA\") and len(running_article) != 0:\n",
    "        article_prediction.append(running_article)\n",
    "        running_article = []\n",
    "    else: \n",
    "        running_article.append(line)\n",
    "article_prediction.append(running_article)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('787966255',\n",
       " \"Warrants Show Police Never Searched Amber Guyger’s Apartment, Now It’s Too Late, She’s ‘Vacated’ It  Dallas, TX — On Sunday, activists rallied around the Dallas Cowboy’s stadium to draw attention to the slaying of Botham Jean. They carried coffins and blocked traffic to expose the special treatment of Amber Guyger, the cop who has yet to even be fired for killing Jean. Now, as more details emerge, this special treatment has moved to a level that is even more insidious. Protest at AT&T Stadium pic.twitter.com/lu5VsGQByv — Allison Harris (@AllisonFox4News) September 16, 2018 take our poll - story continues below Who should replace Nikki Haley as our ambassador to the U.N.? Who should replace Nikki Haley as our ambassador to the U.N.? Who should replace Nikki Haley as our ambassador to the U.N.? * John Bolton Richard Grenell Dina Powell Heather Nauert Ivanka Trump Email * Email This field is for validation purposes and should be left unchanged. Completing this poll grants you access to Freedom Outpost updates free of charge. You may opt out at anytime. You also agree to this site's Privacy Policy and Terms of Use. From the beginning of this case, it was clear that Guyger was receiving special treatment from law enforcement. One of the most glaring instances showing her privilege was the fact that the Texas Rangers said in their report that Jean refused to obey Guyger’s “verbal commands.” Had anyone else but a cop walked into an apartment and killed an innocent man, do you believe that investigators would have said anything like this? Attorney Stephen Le Brocq, who operates a law firm in the North Texas area sums up the treatment of Guyger perfectly when he says that “The affidavit isn’t written objectively, not at the slightest. As stated before, they are usually wholly against the accused. An objective view of the information related by the officer would be proper, in my opinion. This affidavit states facts based upon the word of the person it seeks to accuse of criminal wrongdoing.” What’s more, according to WFAA, who obtained affidavits for all of the search warrants involved in the case, none of them involved the search of Guyger’s apartment. For those who don’t recall, police egregiously released the search warrant findings of Jean’s apartment on the day his family buried his dead body. They attempted to assassinate Jean’s character by claiming they found less than a half ounce of weed in his apartment. However, they failed miserably at it. One thing in particular, however, stood out to those paying attention and that is the fact the police didn’t release any information on what they found in Guyger’s apartment. According to the information currently available, that’s because they didn’t search it. According to WFAA, there were five search warrants issued in the case—none of which were for the killer’s apartment. As WFAA reports: Two of the warrants allowed investigators to remove the front door of Guyger and Jean’s apartment, their door locks and to download data for their door locks. An inventory return states that they removed both of their door lock and downloaded the data from the door locks. A third search warrant gave investigators the authority to enter Jean’s apartment and collect additional evidence. A return shows that investigators took photographs of his apartment, made videos of his apartment, conducted “laser measurements of firearm trajectory,” and collected “gunshot residue” from the door frame and kitchen wall of Jean’s apartment. A return for the fourth search warrant shows investigators seized video from the surveillance camera system in the apartment management’s office. A fifth search warrant gave investigators the authority to obtain all communications related to the incident in the possession of property management, as well as all surveillance video and all entry and access logs from 9 p.m. to 11 p.m. on the night of the shooting. An inventory return shows investigators seized a USB drive containing the video, an event log report for “linear access doors and gates,” an “elevator access door lock” report and a “lock audit report for both apartments. Now, even if police would try to search the killer cop’s apartment, it’s too late. On Sunday, apartment staff notified residents in an email that Guyger “has vacated her apartment and no longer resides at our community.” A manager at the complex declined to indicate exactly when Guyger moved out and if she left the premises voluntarily, the Dallas Morning News reports. While residents can certainly sleep better knowing that they don’t have to fear a killer cop walking into their apartment and murdering them, the family of Botham Jean likely sees this as one more instance of injustice. Mother of #BothamJean, Allison Jean, says the “smear” of her son is unacceptable. “It is time that we recognize that lives matter, my son’s life matters.” pic.twitter.com/UjyKRZ8XeG — Jack Highberger (@JackHighberger) September 14, 2018 Article posted with permission from The Free Thought Project \")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_content_tuple[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After joining back the words into sentences and articles , we have make labels from word level to char level , for example :\n",
    "\n",
    " (\"Trump\",\"O\")\n",
    " should be converted to \n",
    " (\"Trump,\"OOOOO\")\n",
    " \n",
    " This is necessary as we have to find the start span char index and end span char index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def create_equi_length_seq(line):\n",
    "    cleaned_line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "    token = cleaned_line[0].strip()\n",
    "    if len(token) == 0 or token.startswith(\"SOA\") or token.startswith(\"SOS\") or token.startswith(\"EOA\") or token.startswith(\"EOS\"):\n",
    "        return (\" \",\" \")\n",
    "    token_label = cleaned_line[1].strip()\n",
    "    len_token = len(token)\n",
    "    full_token_label = []\n",
    "    for i in range(0,len_token):\n",
    "        full_token_label.append(token_label)\n",
    "    return (token,\"\".join(full_token_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Few examples of this output is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trust', 'OOOOO')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_equi_length_seq('trust O\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' ', ' ')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_equi_length_seq(' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_sequence_tups = []\n",
    "running_article = []\n",
    "for ap in article_prediction:\n",
    "    for word_tup in ap:\n",
    "        resized_tup = create_equi_length_seq(word_tup)\n",
    "        if (resized_tup[0] != \" \"):\n",
    "            running_article.append(resized_tup)\n",
    "    final_sequence_tups.append(running_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_prediction_seq = []\n",
    "for article_words_tup in final_sequence_tups:\n",
    "    label_seq = []\n",
    "    for word_tup in article_words_tup:\n",
    "        label_seq.append(word_tup[1])\n",
    "    final_prediction_seq.append(\" \".join(label_seq))\n",
    "    label_seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_final_sequence = list(zip(article_ids,final_prediction_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Once we have turned the char level , we have to get the propaganda spans from each article ,\n",
    "the following function does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_spans_from_label_seq(article_id_label_seq_tup):\n",
    "    article_id = article_id_label_seq_tup[0]\n",
    "    label_sequence = article_id_label_seq_tup[1]\n",
    "    spans = []\n",
    "    current_running_start_span = None\n",
    "    is_span_running = False\n",
    "    for ix,label in enumerate(label_sequence):\n",
    "        if is_span_running:\n",
    "            if label == \"O\":\n",
    "                spans.append((current_running_start_span,ix-1))\n",
    "                current_running_start_span = None\n",
    "                is_span_running = False\n",
    "        else:\n",
    "            if label == \"I\":\n",
    "                current_running_start_span = ix\n",
    "                is_span_running = True\n",
    "    return article_id, spans            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('779309765', [(158816, 159110), (188194, 188281), (335245, 335661)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spans_from_label_seq(article_final_sequence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally create a submission file and write it into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_submission_list =  [get_spans_from_label_seq(article_tup) for article_tup in article_final_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_submission_strings = []\n",
    "for fsl in final_submission_list:\n",
    "    article_id = fsl[0]\n",
    "    for span in fsl[1]:\n",
    "        final_submission_strings.append(str(article_id) + \"\\t\" + str(span[0]) + \"\\t\" + str(span[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_submission_content = \"\\n\".join(final_submission_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "final_submission_content = \"id\\tbegin_offset\\tend_offset\\n\" + final_submission_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open(\"final_submission.txt\",\"w\") as f:\n",
    "    f.writelines(final_submission_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Compressing the output due to file size limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The idea of compression is , for a article , if two spans are close to each other by n chars , we can combine them into one span , For the n char \n",
    "various values was tried and span_diff of 310 was just right to make the file size less 500 kb / 22000 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_spans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-de7caf2df002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspan_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m310\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_article_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_span\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'article_spans' is not defined"
     ]
    }
   ],
   "source": [
    "span_diff = 310\n",
    "new_article_spans = []\n",
    "for ix, article_span in enumerate(article_spans):\n",
    "    if ix == 0:\n",
    "        pass\n",
    "    elif ix==1:\n",
    "        new_article_spans.append(article_span.split(\"\\t\"))\n",
    "    else:\n",
    "        next_span_start_position =  int(article_span.split(\"\\t\")[1])\n",
    "        prev_span_end_position = int(new_article_spans[-1][2])\n",
    "        next_article_id =  int(article_span.split(\"\\t\")[0])\n",
    "        prev_article_id = int(new_article_spans[-1][0])        \n",
    "#         print(prev_span_end_position,next_span_start_position)\n",
    "        if (next_article_id == prev_article_id):\n",
    "            if (next_span_start_position - prev_span_end_position) < span_diff:\n",
    "                to_insert_span = [new_article_spans[-1][0],new_article_spans[-1][1],article_span.split(\"\\t\")[2]]\n",
    "                new_article_spans.pop()\n",
    "                new_article_spans.append(to_insert_span)\n",
    "            else:\n",
    "                new_article_spans.append(article_span.split(\"\\t\"))\n",
    "        else:\n",
    "            new_article_spans.append(article_span.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(len(new_article_spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "article_span_strings = []\n",
    "for article_span in new_article_spans:\n",
    "    article_span_strings.append(\"\\t\".join(article_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "condensed_final_submission = \"\".join(article_span_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open(\"condensed_final_submission.txt\",\"w\") as f:\n",
    "    f.writelines(condensed_final_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval2020",
   "language": "python",
   "name": "semeval2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
